package fr.mosef.scala.template

import org.apache.spark.sql.{DataFrame, SparkSession}
import fr.mosef.scala.template.processor.Processor
import fr.mosef.scala.template.processor.impl.ProcessorImpl
import fr.mosef.scala.template.reader.Reader
import fr.mosef.scala.template.reader.impl.ReaderImpl
import fr.mosef.scala.template.writer.Writer
import org.apache.spark.SparkConf
import com.globalmentor.apache.hadoop.fs.BareLocalFileSystem
import org.apache.hadoop.fs.FileSystem

import java.util.Properties
import java.io.{FileInputStream, InputStream}

object Main extends App {

  val cliArgs = args

  val MASTER_URL: String = try cliArgs(0) catch {
    case _: ArrayIndexOutOfBoundsException => "local[1]"
  }

  val SRC_PATH: String = try cliArgs(1) catch {
    case _: ArrayIndexOutOfBoundsException =>
      println("‚ùå Aucune source d'entr√©e d√©finie")
      sys.exit(1)
  }

  val DST_PATH: String = try cliArgs(2) catch {
    case _: ArrayIndexOutOfBoundsException => "./default/output-writer"
  }

  val REPORT_TYPES: Seq[String] = try cliArgs(3).split(",").map(_.trim).toSeq catch {
    case _: ArrayIndexOutOfBoundsException =>
      println("‚ö†Ô∏è Aucun type de rapport pr√©cis√©, 'report1' utilis√© par d√©faut")
      Seq("report1")
  }

  val CONFIG_PATH: Option[String] = if (cliArgs.length > 4) Some(cliArgs(4)) else None

  val conf = new SparkConf()
  conf.set("spark.driver.memory", "2g")
  conf.set("spark.testing.memory", "471859200")

  val sparkSession = SparkSession
    .builder
    .master(MASTER_URL)
    .config(conf)
    .appName("Scala Template")
    .enableHiveSupport()
    .getOrCreate()

  sparkSession.sparkContext.hadoopConfiguration.setClass(
    "fs.file.impl",
    classOf[BareLocalFileSystem],
    classOf[FileSystem]
  )

  def detectFormatFromPath(path: String): String = {
    val lower = path.toLowerCase
    if (lower.endsWith(".csv")) "csv"
    else if (lower.endsWith(".parquet")) "parquet"
    else if (lower.startsWith("hive:")) "hive"
    else "unknown"
  }

  // Chargement automatique de la config
  val confWriter = new Properties()
  val stream: InputStream = CONFIG_PATH match {
    case Some(path) =>
      println(s"Chargement de la configuration externe : $path")
      new FileInputStream(path)
    case None =>
      println("Chargement de la configuration interne : configuration.properties (dans resources)")
      val resourceStream = getClass.getClassLoader.getResourceAsStream("configuration.properties")
      if (resourceStream == null) {
        throw new RuntimeException("‚ùå Fichier de configuration interne introuvable !")
      }
      resourceStream
  }

  confWriter.load(stream)

  val reader: Reader = new ReaderImpl(sparkSession)
  val processor: Processor = new ProcessorImpl()
  val writer: Writer = new Writer(sparkSession, confWriter)

  val format = detectFormatFromPath(SRC_PATH)
  println(s"\nFormat d√©tect√©: $format")
  println(s"Lecture des donn√©es depuis : $SRC_PATH")

  val inputDF: DataFrame = format match {
    case "csv"     => reader.readCSV(SRC_PATH, delimiter = ",", header = true)
    case "parquet" => reader.readParquet(SRC_PATH)
    case "hive"    => reader.readHiveTable(SRC_PATH.stripPrefix("hive:"))
    case _ =>
      println(s"‚ùå Format inconnu pour le chemin : $SRC_PATH")
      sys.exit(1)
  }

  println(s"‚úÖ Lecture termin√©e. Nombre de lignes : ${inputDF.count()}")
  println(s"Aper√ßu des donn√©es d'entr√©e :")
  inputDF.show(10, truncate = false)

  REPORT_TYPES.foreach { report =>
    println(s"\nTraitement du rapport : '$report' en cours...")
    val processedDF = processor.process(inputDF, report)

    println(s"‚úÖ Rapport '$report' g√©n√©r√© avec succ√®s.")
    println(s"Aper√ßu du rapport '$report' :")
    writer.showDataFrame(processedDF, numRows = 10)

    val outputPath = s"$DST_PATH/$report"
    println(s"√âcriture du rapport '$report' vers : $outputPath")
    writer.write(processedDF, outputPath)
    println(s"Rapport '$report' √©crit avec succ√®s.\n")
  }

  println("üéâ Tous les rapports ont √©t√© trait√©s et sauvegard√©s avec succ√®s !")
}